{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9xout896BIh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import gensim.downloader as gensim_downloader\n",
        "import numpy as np\n",
        "import gensim\n",
        "\n",
        "import re  # For preprocessing\n",
        "import pandas as pd  # For data handling\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "\n",
        "import spacy  # For preprocessing\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "#automatically detect common phrases (bigrams) from a list of sentences.\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models import Word2Vec\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import string\n",
        "from Utils import Utils\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from collections import Counter\n",
        "\n",
        "class DataCleaning(object):\n",
        "\t#Testing data embedding is also stored in here, as we are using same model for traing\n",
        "\t#Lambda Functions - Start\n",
        "\tdef cleanString(self, text: str, ifOutlireCharRemoveNeeded = False)-> str:\n",
        "\t\t#remove punctuation\n",
        "\t\ttext = \"\".join([char for char in text if char not in string.punctuation])\n",
        "\t\tif ifOutlireCharRemoveNeeded:\n",
        "\t\t\ttext = \"\".join([char for char in text if char not in ['N', 'K', 'M', 'R', 'S', 'W', 'Y']])\n",
        "\t\t# Convert all to lower\n",
        "\t\t\t#not needed in here\n",
        "\t\t# Remove Stop Words\n",
        "\t\t\t#not needed in here\n",
        "\t\treturn text\n",
        "\n",
        "\tdef appendCounterDict(self, wordDict: dict)-> None:\n",
        "\t\tfor k in wordDict:\n",
        "\t\t\tself.wordCounter[k]+=wordDict[k]\n",
        "\t\treturn\n",
        "\n",
        "\tdef splitWords(self, dna_seq: str, word_len: int)-> str:\n",
        "\t\tstrArray = [dna_seq[index : index + word_len] for index in range(0, len(dna_seq), word_len)]\n",
        "\t\tself.totalUniqueWords.update(strArray)\n",
        "\t\tself.appendCounterDict(dict(Counter(strArray)))\n",
        "\t\ttext = \" \".join(strArray)\n",
        "\t\tself.maxWordLen = max(self.maxWordLen, len(strArray))\n",
        "\t\treturn text\n",
        "\t#Lambda Functions - End\n",
        "\n",
        "\tdef __init__(self, train_x_file = \"train_features.csv\", train_y_file = \"train_labels.csv\", test_x_file = \"test_features.csv\"):\n",
        "\t\tdataX = pd.read_csv(Utils.getAbsFilePath(train_x_file), index_col=0)\n",
        "\t\tself.maxWordLen = 0\n",
        "\t\tself.total_data = dataX\n",
        "\t\tdataY = pd.read_csv(Utils.getAbsFilePath(train_y_file), index_col=0)\n",
        "\t\tself.total_data['labels'] = dataY['labels']\n",
        "\t\tself.lebels = set(dataY['labels'])\n",
        "\t\tself.X_pred = pd.read_csv(Utils.getAbsFilePath(test_x_file), index_col=0)\n",
        "\t\tself.totalUniqueWords = set()\n",
        "\t\tself.wordCounter = defaultdict(int)\n",
        "\n",
        "\tdef upDownScale(self, ratio = 10) -> None:\n",
        "\t\t#Upscale data with lowest frequency or upscale data with highest frequency\n",
        "\t\tval_count = self.total_data['labels'].value_counts()\n",
        "\t\t#Is needed if data ratio is like 50:1 or 100:1 we need any data upscale, downscale or augmentation\n",
        "\t\t#for our data, we are doing upscale - augmentation as highest count is 137 and lowest one is 2, where every categorised data should be at least 137/15 ~ 10\n",
        "\t\tlabel_index = defaultdict(list)\n",
        "\t\tfor index, row in self.total_data.iterrows():\n",
        "\t\t\ttry:\n",
        "\t\t\t\tlabel_index[self.total_data.at[index, 'labels']].append(index)\n",
        "\t\t\texcept Exception as err:\n",
        "\t\t\t\tprint(f'Error occurred during updating row of train_dna: {err}')\n",
        "\t\tmarkerCountNumber = math.ceil(val_count.max()/ratio)\n",
        "\t\tprint(\"Before Upscale/Augmentation-\" + str(self.total_data.shape))\n",
        "\t\tfor label, count in self.total_data['labels'].value_counts().iteritems():\n",
        "\t\t\tself.upScale(label_index[label], markerCountNumber-count)\n",
        "\t\tprint(\"After Upscale/Augmentation- \" + str(self.total_data.shape))\n",
        "\t\treturn\n",
        "\n",
        "\tdef upScale(self, id_list, no_of_new_data) -> None:\n",
        "\t\tif no_of_new_data<0: return\n",
        "\t\tfor id_to_augment in random.choices(id_list, k = no_of_new_data):\n",
        "\t\t\t#Augmentation is not implemented here, just duplicating data is done in here\n",
        "\t\t\tself.total_data = self.total_data.append({\n",
        "\t\t\t\t\t\t'labels' : self.total_data.at[id_to_augment, 'labels'],\n",
        "\t\t\t\t\t\t'dna' : self.total_data.at[id_to_augment, 'dna']\n",
        "\t\t\t\t\t},\n",
        "\t\t\t\t\tignore_index = True\n",
        "\t\t\t\t)\n",
        "\t\treturn None\n",
        "\n",
        "\tdef clean(self, ifOutlireCharRemoveNeeded = False) -> None:\n",
        "\t\tself.total_data['dna'] = self.total_data['dna'].apply(lambda x: self.cleanString(x, ifOutlireCharRemoveNeeded))\n",
        "\t\tself.X_pred['dna'] = self.X_pred['dna'].apply(lambda x: self.cleanString(x, ifOutlireCharRemoveNeeded))\n",
        "\t\treturn\n",
        "\n",
        "\tdef preprocess(self, word_len = 4) -> None:\n",
        "\t\t# Y don't need to be preprocessed because it is already set to numeric values\n",
        "\t\tself.total_data['dna'] = self.total_data['dna'].apply(lambda x: self.splitWords(x, word_len))\n",
        "\t\tself.X_pred['dna'] = self.X_pred['dna'].apply(lambda x: self.splitWords(x, word_len))\n",
        "\t\tprint(\"Max word in a scentence:\", self.maxWordLen)\n",
        "\t\treturn\n",
        "\n",
        "\tdef save(self, file_name = \"input_data.csv\", x_test_file_name = \"x_test.csv\") -> None:\n",
        "\t\tself.total_data.to_csv(Utils.getAbsFilePath(file_name))\n",
        "\t\tself.X_pred.to_csv(Utils.getAbsFilePath(x_test_file_name))\n",
        "\n",
        "\tdef generateSentenceEmbedding(self) -> None:\n",
        "\t\t#sbert\n",
        "\t\tmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\t\t#For Training Data\n",
        "\t\tfor index, row in self.total_data.iterrows():\n",
        "\t\t\ttry:\n",
        "\t\t\t\tembedding = model.encode(self.total_data.at[index, 'dna'])\n",
        "\t\t\t\tfor embadeIndex, val in enumerate(embedding):\n",
        "\t\t\t\t\tself.total_data.at[index, \"sbert_\" + str(embadeIndex).zfill(3)] = val\n",
        "\t\t\texcept Exception as err:\n",
        "\t\t\t\tprint(f'Error occurred during updating row of train_dna: {err}')\n",
        "\t\t#For Test Data\n",
        "\t\tfor index, row in self.X_pred.iterrows():\n",
        "\t\t\ttry:\n",
        "\t\t\t\tembedding = model.encode(self.X_pred.at[index, 'dna'])\n",
        "\t\t\t\tfor embadeIndex, val in enumerate(embedding):\n",
        "\t\t\t\t\tself.X_pred.at[index, \"sbert_\" + str(embadeIndex).zfill(3)] = val\n",
        "\t\t\texcept Exception as err:\n",
        "\t\t\t\tprint(f'Error occurred during updating row of X_pred_dna: {err}')\n",
        "\t\tself.X_pred.drop([\"dna\"], axis=1, inplace= True, errors='ignore')\n",
        "\t\treturn\n",
        "\n",
        "\tdef generateWord2VecEmbedding(self, vector_size=200, window=20, epochs=100, min_count=2) -> None:\n",
        "\t\t# Help is in here- https://github.com/Amrit27k/NLPword2vec/blob/master/word2vec-on-rick-morty-dataset.ipynb\n",
        "\t\t# And help is in here- https://www.linkedin.com/learning/advanced-nlp-with-python-for-machine-learning/how-to-prep-word-vectors-for-modeling?u=87254282\n",
        "\t\tpreprocessed_data = self.total_data['dna'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
        "\t\tw2v_model = gensim.models.Word2Vec(preprocessed_data,\n",
        "\t\t\t\t\t\t\t\t   vector_size=vector_size,\n",
        "\t\t\t\t\t\t\t\t   window=window,\n",
        "\t\t\t\t\t\t\t\t   min_count=min_count,\n",
        "\t\t\t\t\t\t\t\t   epochs=epochs,\n",
        "\t\t\t\t\t\t\t\t   workers=15,\n",
        "\t\t\t\t\t\t\t\t   sg=1)\n",
        "\n",
        "\t\t# Generate a list of words the word2vec model learned word vectors for\n",
        "\t\tw2v_model.wv.index_to_key\n",
        "\t\t# Generate aggregated sentence vectors based on the word vectors for each word in the sentence\n",
        "\t\tw2v_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in w2v_model.wv.index_to_key])\n",
        "\t\t\t\t\t for ls in self.total_data['dna']])\n",
        "\t\t# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
        "\t\tw2v_vect_avg = []\n",
        "\n",
        "\t\t#Padding the sequence\n",
        "\t\tfor vect in w2v_vect:\n",
        "\t\t\tif len(vect)!=0:\n",
        "\t\t\t\tw2v_vect_avg.append(vect.mean(axis=0))\n",
        "\t\t\telse:\n",
        "\t\t\t\tw2v_vect_avg.append(np.zeros(100))\n",
        "\n",
        "\t\t#Store embedding\n",
        "\t\tfor index, row in self.total_data.iterrows():\n",
        "\t\t\ttry:\n",
        "\t\t\t\tfor embadeIndex, val in enumerate(w2v_vect_avg[index]):\n",
        "\t\t\t\t\tself.total_data.at[index, \"w2vec_\" + str(embadeIndex).zfill(4)] = val\n",
        "\t\t\texcept Exception as err:\n",
        "\t\t\t\tprint(f'Error occurred during updating row of w2vec: {err}')\n",
        "\n",
        "\tdef generateDoc2VecEmbedding(self, vector_size=200, window=20, epochs=100, min_count=2) -> None:\n",
        "\t\t#https://www.linkedin.com/learning/advanced-nlp-with-python-for-machine-learning/how-to-implement-doc2vec?u=87254282\n",
        "\t\ttagged_docs_tr = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(self.total_data['dna'])]\n",
        "\n",
        "\t\td2v_model = gensim.models.Doc2Vec(tagged_docs_tr,\n",
        "\t\t\t\t\t\t\t\t   vector_size=vector_size,\n",
        "\t\t\t\t\t\t\t\t   window=window,\n",
        "\t\t\t\t\t\t\t\t   min_count=min_count,\n",
        "\t\t\t\t\t\t\t\t   epochs=epochs,\n",
        "\t\t\t\t\t\t\t\t   workers=15)\n",
        "\t\t#For Train Data\n",
        "\t\tfor index, row in self.total_data.iterrows():\n",
        "\t\t\ttry:\n",
        "\t\t\t\tembedding = d2v_model.infer_vector(self.total_data.at[index, 'dna'].split())\n",
        "\t\t\t\tfor embadeIndex, val in enumerate(embedding):\n",
        "\t\t\t\t\tself.total_data.at[index, \"d2vec_\" + str(embadeIndex).zfill(4)] = val\n",
        "\t\t\texcept Exception as err:\n",
        "\t\t\t\tprint(f'Error occurred during updating row of d2vec: {err}')\n",
        "\t\t#For Test Data\n",
        "\t\tfor index, row in self.X_pred.iterrows():\n",
        "\t\t\ttry:\n",
        "\t\t\t\tembedding = d2v_model.infer_vector(self.X_pred.at[index, 'dna'].split())\n",
        "\t\t\t\tfor embadeIndex, val in enumerate(embedding):\n",
        "\t\t\t\t\tself.X_pred.at[index, \"d2vec_\" + str(embadeIndex).zfill(4)] = val\n",
        "\t\t\texcept Exception as err:\n",
        "\t\t\t\tprint(f'Error occurred during updating row of d2vec: {err}')\n",
        "\t\tself.X_pred.drop([\"dna\"], axis=1, inplace= True, errors='ignore')\n",
        "\t\treturn\n",
        "\n",
        "\tdef generate4MersEncoding(self, vector_size=200, window=20, epochs=100, min_count=2) -> None:\n",
        "\t\tmlb = MultiLabelBinarizer()\n",
        "\t\t#self.totalUniqueWords\t#=> all columns for words\n",
        "\t\tfor word in self.totalUniqueWords:\n",
        "\t\t\tself.total_data['onehot_' + word] = 0\n",
        "\t\t\tself.X_pred['onehot_' + word] = 0\n",
        "\n",
        "\t\t#For Train Data\n",
        "\t\tdf = pd.DataFrame(mlb.fit_transform([x.split(' ') for x in self.total_data[\"dna\"]]),columns=mlb.classes_)\n",
        "\t\t#df.to_csv(Utils.getAbsFilePath(\"one_hot.csv\"))\n",
        "\t\tfor index, row in self.total_data.iterrows():\n",
        "\t\t\ttry:\n",
        "\t\t\t\tfor word in df.keys():\n",
        "\t\t\t\t\tself.total_data.at[index, \"4mers_\" + word] = df.at[index, word]\n",
        "\t\t\texcept Exception as err:\n",
        "\t\t\t\tprint(f'Error occurred during updating row of onehot_train: {err}')\n",
        "\n",
        "\t\t#For Test Data\n",
        "\t\tdf = pd.DataFrame(mlb.fit_transform([x.split(' ') for x in self.X_pred[\"dna\"]]),columns=mlb.classes_)\n",
        "\t\tfor index, row in self.X_pred.iterrows():\n",
        "\t\t\ttry:\n",
        "\t\t\t\tfor word in df.keys():\n",
        "\t\t\t\t\tself.X_pred.at[index, \"4mers_\" + word] = df.at[index, word]\n",
        "\t\t\texcept Exception as err:\n",
        "\t\t\t\tprint(f'Error occurred during updating row of onehot_pred: {err}')\n",
        "\t\tself.X_pred.drop([\"dna\"], axis=1, inplace= True, errors='ignore')\n",
        "\t\treturn\n",
        "\n",
        "\tdef generateOneHotEncoding(self, vector_size=200, window=20, epochs=100, min_count=2) -> None:\n",
        "\t\t#Should always done with 1 char and every values\n",
        "\t\t#So, perWordLength = 1 should be set in .env file for this\n",
        "\t\tfor wordIndex in range(self.maxWordLen):\t#For fixing padding\n",
        "\t\t\tfor word in self.totalUniqueWords:\n",
        "\t\t\t\tself.total_data['onehot_' + str(wordIndex) + word] = 0\n",
        "\t\t\t\tself.X_pred['onehot_' + str(wordIndex) + word] = 0\n",
        "\t\t#For training Data\n",
        "\t\tfor index, row in self.total_data.iterrows():\n",
        "\t\t\twords = self.total_data.at[index, 'dna'].split()\n",
        "\t\t\tfor i, w in enumerate(words):\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\tcoumnName = 'onehot_' + str(i) + w\n",
        "\t\t\t\t\tself.total_data.at[index, coumnName] = 1\n",
        "\t\t\t\texcept Exception as err:\n",
        "\t\t\t\t\tprint(f'Error occurred during updating encoding_row_train of onehot_train: {err}')\n",
        "\t\t#For test Data\n",
        "\t\tfor index, row in self.X_pred.iterrows():\n",
        "\t\t\twords = self.X_pred.at[index, 'dna'].split()\n",
        "\t\t\tfor i, w in enumerate(words):\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\tcoumnName = 'onehot_' + str(i) + w\n",
        "\t\t\t\t\tself.X_pred.at[index, coumnName] = 1\n",
        "\t\t\t\texcept Exception as err:\n",
        "\t\t\t\t\tprint(f'Error occurred during updating encoding_row_pred of onehot_train: {err}')\n",
        "\t\tself.X_pred.drop([\"dna\"], axis=1, inplace= True, errors='ignore')\n",
        "\t\treturn\n",
        "\n",
        "\tdef getTrainTestSplit(self, file_name = \"input_data.csv\", embedding = \"sbert\"):\n",
        "\t\tself.total_data = pd.read_csv(Utils.getAbsFilePath(file_name), index_col=0)\n",
        "\t\ttrain, test = train_test_split(self.total_data, test_size=0.2)\n",
        "\t\ttrain = self.total_data\n",
        "\n",
        "\t\ty_tr =  train[['labels']]\n",
        "\t\ty_test = test[['labels']]\n",
        "\t\tif embedding==\"sbert\":\t#paraphrase-MiniLM-L6-v2_embedding\n",
        "\t\t\tX_tr = train[[s for s in train.columns if \"sbert_\" in s]]\n",
        "\t\t\tX_test = test[[s for s in test.columns if \"sbert_\" in s]]\n",
        "\t\telif embedding==\"w2vec\":\n",
        "\t\t\tX_tr = train[[s for s in train.columns if \"w2vec_\" in s]]\n",
        "\t\t\tX_test = test[[s for s in test.columns if \"w2vec_\" in s]]\n",
        "\t\telif embedding == \"d2vec\":\n",
        "\t\t\tX_tr = train[[s for s in train.columns if \"d2vec_\" in s]]\n",
        "\t\t\tX_test = test[[s for s in test.columns if \"d2vec_\" in s]]\n",
        "\t\telif embedding == \"4mers\":\n",
        "\t\t\tX_tr = train[[s for s in train.columns if \"4mers_\" in s]]\n",
        "\t\t\tX_test = test[[s for s in test.columns if \"4mers_\" in s]]\n",
        "\t\telif embedding == \"onehot\":\n",
        "\t\t\tX_tr = train[[s for s in train.columns if \"onehot_\" in s]]\n",
        "\t\t\tX_test = test[[s for s in test.columns if \"onehot_\" in s]]\n",
        "\t\telse:\n",
        "\t\t\tX_tr = train[['dna']]\n",
        "\t\t\tX_test = test[['dna']]\n",
        "\t\treturn (X_tr,y_tr), (X_test,y_test)\n",
        "\n",
        "\tdef getXTest(self, file_name = \"x_test.csv\", embedding = \"sbert\"):\n",
        "\t\tself.X_pred = pd.read_csv(Utils.getAbsFilePath(file_name), index_col=0)\n",
        "\t\tif embedding==\"sbert\":\t#paraphrase-MiniLM-L6-v2_embedding\n",
        "\t\t\tX_pred = self.X_pred[[s for s in self.X_pred.columns if \"sbert_\" in s]]\n",
        "\t\telif embedding==\"w2vec\":\n",
        "\t\t\tX_pred = self.X_pred[[s for s in self.X_pred.columns if \"w2vec_\" in s]]\n",
        "\t\telif embedding == \"d2vec\":\n",
        "\t\t\tX_pred = self.X_pred[[s for s in self.X_pred.columns if \"d2vec_\" in s]]\n",
        "\t\telif embedding == \"4mers\":\n",
        "\t\t\tX_pred = self.X_pred[[s for s in self.X_pred.columns if \"4mers_\" in s]]\n",
        "\t\telif embedding == \"onehot\":\n",
        "\t\t\tX_pred = self.X_pred[[s for s in self.X_pred.columns if \"onehot_\" in s]]\n",
        "\t\telse:\n",
        "\t\t\tX_pred = self.X_pred[['dna']]\n",
        "\t\treturn X_pred"
      ]
    }
  ]
}